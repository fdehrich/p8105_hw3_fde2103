---
title: "Homework 3"
author: Fiona Ehrich
output: github_document
---

```{r setup, include = FALSE}
library(tidyverse)
library(p8105.datasets)

knitr::opts_chunk$set(
	fig.width = 6, 
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

```{r}
data("instacart")
```

This datasets contains `r nrow(instacart)` rows and `r ncol(instacart)` columns. Observations are the level of items in orders by user. There are user / order variables -- user ID, order ID, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric codes. There are various aisles within a given department.

There are `r instacart %>% count(aisle) %>% nrow()` aisles. The table below shows that most items are from the following aisles: "fresh vegetables", "fresh fruits", and "packaged vegetables fruits".

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

Below is a plot that shows the number of items ordered in each aisle, limited to aisles with more than 10000 items ordered.

```{r}
instacart %>% 
	count(aisle) %>% 
	filter(n > 10000) %>% 
	mutate(
		aisle = factor(aisle),
		aisle = fct_reorder(aisle, n)
	) %>% 
	ggplot(aes(x = aisle, y = n)) + 
	geom_point() + 
	theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

Below is a table showing the three most popular items in each of the aisles "baking ingredients", "dog food care", "packaged vegetables fruits".

```{r}
instacart %>% 
	filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
	group_by(aisle) %>% 
	count(product_name) %>% 
	mutate(rank = min_rank(desc(n))) %>% 
	filter(rank < 4) %>% 
	arrange(aisle, rank) %>% 
	knitr::kable()
```

Below is a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.

```{r message = FALSE, warning = FALSE}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) %>% 
  knitr::kable()
```

## Problem 2

Load, tidy, and otherwise wrangle the data.

```{r}
accel =
  read_csv(
    "./data/accel_data.csv",
    col_types = cols( # I am making sure these variables are in appropriate classes
      week = col_factor(),
      day_id = col_factor(),
      day = col_factor()
      )
    ) %>% 
  janitor::clean_names() %>% 
  pivot_longer( # Making the dataset more tidy
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    names_transform = list(minute = as.numeric), # Making the minute variable numeric
    values_to = "activity",
    ) %>% 
  mutate(
    day = forcats::fct_relevel(day,c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")), # Putting the day of the week factors in a sensible order
    day_type = # Making a new variable to indicate whether weekend or weekday
      as.factor(
        ifelse(day == c("Saturday", "Sunday"), "weekend", "weekday")
        )
    )
```

The resulting dataset has `r nrow(accel)` rows and `r ncol(accel)` columns. The variables are: `r names(accel)`. This dataset is now tidier than it was originally; each activity observation now exists on its own row.

Now, I will aggregate across minutes to create a total activity for each day. Below is a table that displays the total activity for each day in chronological order (I sorted first by week and then by day of the week). I put this table into a "wider" format so it is easier to read.

```{r message = FALSE, warning = FALSE}
accel %>% 
  group_by(week, day) %>% 
  summarize(total_activity = sum(activity)) %>% 
  pivot_wider(
    names_from = day,
    values_from = total_activity) %>% 
  knitr::kable()
```

It is hard to see any particular trends from this table alone. Just for my own knowledge, I created a "longer" version of this table and sorted from highest to lowest total activity per day (below). I can see that the maximum total activity per day is 685910.00 and the minimum total activity per day is 1440.00 (and I can see how each day ranks in terms of total activity).

```{r message = FALSE, warning = FALSE}
accel %>% 
  group_by(week, day) %>% 
  summarize(total_activity = sum(activity)) %>% 
  arrange(desc(total_activity)) %>% 
  knitr::kable()
```

I am now making a plot that shows the 24-hour activity time courses for each day.

```{r message = FALSE, warning = FALSE}
accel %>% 
  ggplot(aes(x = minute, y = activity, color = day)) + 
  geom_smooth(se = FALSE) +
  labs(
    title = "24-Hour Activity Time Course by Day of the Week (Accelerometer Data)",
    x = "Minute of the Day",
    y = "Activity Count",
    color = "Day of the Week"
  ) +
  scale_x_continuous(
    breaks = c(1, 361, 721, 1081),
    labels = c("12 am", "6 am", "12 pm", "6 pm")
  )
```

From this plot, I can see that activity tends to be lower at night (presumably while the person wearing the accelerometer is resting or sleeping) and higher during the day. I also notice some peaks in activity on Sunday late mornings and on Friday evenings. I think that these peaks could correspond to some of this person's regular, weekly activities (for example, going out on the town on Friday nights!).

## Problem 3

```{r}
data("ny_noaa")
```

This dataset contains `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. The variable names are `r names(ny_noaa)`. Essentially, each row contains various weather-related observations for a given weather station on a given day (precipitation, snowfall, snow depth, maximum temperature, minimum temperature). By using the summary function (output below), we can see there is data missing for all five of the weather-related variables.

```{r}
ny_noaa %>% 
  mutate( # I am just quickly making tmax and tmin numeric here so that the summary displays properly
    tmax = as.numeric(tmax),
    tmin = as.numeric(tmin)
  ) %>% 
  summary()
```

To get arough sense of the extent of this missing data, the following are the proportions of missing data relative to the total number of rows (I evaluated this for each of the weather-related variables): `r sum(is.na(pull(ny_noaa, prcp)))/nrow(ny_noaa)` for precipitation, `r sum(is.na(pull(ny_noaa, snow)))/nrow(ny_noaa)` for snowfall, `r sum(is.na(pull(ny_noaa, snwd)))/nrow(ny_noaa)` for snow depth, `r sum(is.na(pull(ny_noaa, tmax)))/nrow(ny_noaa)` for maximum temperature, and `r sum(is.na(pull(ny_noaa, tmin)))/nrow(ny_noaa)` for minimum temperature. These values confirm that there is quite a bit of missing data in this dataset, particularly for the temperature variables.

Now, I will clean the data. I am splitting up the date variable, making sure variables are in appropriate classes, and converting certain variables to more appropriate units.

```{r}
ny_noaa_tidy =
  ny_noaa %>%
  separate(date, into = c("year", "month", "day"), sep = "-") %>% # Separating the date variable
   mutate(
     year = as.integer(year), # Correcting the variable classes
     month = as.integer(month),
     day = as.integer(day),
     tmax = (as.numeric(tmax))/10, # Converting this into degrees C rather than tenths of degrees C
     tmin = (as.numeric(tmin))/10, # Converting this into degrees C rather than tenths of degrees C
     prcp = prcp/10 # Converting this into mm rather than tenths of mm
   )
```

The table below shows the most commonly observed values. We can see that 0 is the most commonly observed value. This makes sense as snow does not typically occur in NY for a majority of the months.

```{r}
ny_noaa_tidy %>% 
  count(snow) %>% 
  arrange(desc(n))
```

